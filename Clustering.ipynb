{"cells": [{"cell_type": "code", "execution_count": 1, "id": "6ebeaf4b-feba-4d3b-91f9-91cf7a72efc4", "metadata": {}, "outputs": [], "source": "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import DataFrame"}, {"cell_type": "code", "execution_count": 2, "id": "b0be1dc0-fc8a-45cc-9ba6-306fa69f032c", "metadata": {}, "outputs": [], "source": "spark = SparkSession.builder.getOrCreate()\nspark.conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")"}, {"cell_type": "code", "execution_count": 4, "id": "202f9b53-ff7a-4bbd-a99d-2892b611c9c1", "metadata": {}, "outputs": [], "source": "df_cluster = spark.read.parquet(\"gs://msca-bdp-project-goodreads/clustering_desc_v1.parquet\")"}, {"cell_type": "code", "execution_count": 25, "id": "afe0974d-230e-4e51-ba4c-25a1348b659d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "1573789"}, "execution_count": 25, "metadata": {}, "output_type": "execute_result"}], "source": "#df_cluster.select('description').count()"}, {"cell_type": "code", "execution_count": 5, "id": "8cb185a8-a9e4-4a50-aeea-719dad8ba224", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 4:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------+--------------------+\n| book_id|         description|\n+--------+--------------------+\n|26331592|\"Some people say,...|\n| 2307748|Many of the world...|\n|13022011|When The Clyde Ra...|\n|13249442|Koskinen had retu...|\n|13171109|Paul and Angela K...|\n|   59992|Wolverine's vacat...|\n| 1186942|For anyone lookin...|\n| 1123543|Experience the Ma...|\n| 1123542|The noted artist ...|\n|  370426|One of the more i...|\n|25175834|Dirty\nA poem by B...|\n| 1123549|\"I am Johannes Ve...|\n|  370429|Henry David Thore...|\n|35577588|Exploding from th...|\n|17833502|Rand figures out ...|\n|16137105|Latoa County was ...|\n| 6676766|Ghostly diners, v...|\n| 6676765|From Brierley Hil...|\n|  340853|In this luminous ...|\n|  340854|In the soothing d...|\n+--------+--------------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#df_cluster.show()"}, {"cell_type": "code", "execution_count": 5, "id": "5b5e79b2-ddb1-4a8c-91d1-4de824e9e789", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- book_id: string (nullable = true)\n |-- description: string (nullable = true)\n\n"}], "source": "#df_cluster.printSchema()"}, {"cell_type": "code", "execution_count": 5, "id": "457a1132-2832-4123-88c3-2aaf1e52b602", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import StringType\n\ndef remove_urls(text):\n    import re\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub('', text)\n\ndef remove_symbols(text):\n    symbols_to_remove = ['.', ',', '!', '@', '-', \"'\", '\"', '*', '?', '~', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n    for symbol in symbols_to_remove:\n        text = text.replace(symbol, '')\n    return text\n\nremove_urls_udf = udf(remove_urls, StringType())\ndf_cluster = df_cluster.withColumn(\"desc_wo_urls\", remove_urls_udf(col(\"description\")))\n\nremove_symbols_udf = udf(remove_symbols, StringType())\ndf_cluster = df_cluster.withColumn(\"desc_wo_symbols\", remove_symbols_udf(col(\"desc_wo_urls\")))\n\nto_drop = ['desc_wo_urls', 'description']\ndf_cluster = df_cluster.drop(*to_drop)"}, {"cell_type": "code", "execution_count": 6, "id": "773af2ae-faf2-4acc-9b98-c0aeefb9fe98", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n# Tokenize and remove stop words\ntokenizer = Tokenizer(inputCol=\"desc_wo_symbols\", outputCol=\"words\")\nstop_words_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n\n# Create a pipeline\npipeline = Pipeline(stages=[tokenizer, stop_words_remover])\n\n# Fit the pipeline to the DataFrame\npipeline_model = pipeline.fit(df_cluster)\ndf_cluster = pipeline_model.transform(df_cluster)\n\n# Hashing Term Frequency (TF)\nhashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"hashing_tf_features\")\ntf_data = hashing_tf.transform(df_cluster)\n\n# Inverse Document Frequency (IDF)\nidf = IDF(inputCol=\"hashing_tf_features\", outputCol=\"features\")\nidf_model = idf.fit(tf_data)\nvectorized_data = idf_model.transform(tf_data)"}, {"cell_type": "code", "execution_count": null, "id": "1f8f6562-d131-426f-9e7a-fc208350c476", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/11/26 22:33:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:34:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:34:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:34:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:34:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:34:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:34:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:34:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:34:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:34:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:34:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:01 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:02 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n23/11/26 22:35:11 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 22.1 MiB\n23/11/26 22:35:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 22.1 MiB\n23/11/26 22:35:42 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 24.1 MiB\n23/11/26 22:36:14 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n23/11/26 22:36:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 22.1 MiB\n                                                                                \r"}], "source": "from pyspark.ml.feature import Tokenizer, StopWordsRemover, RegexTokenizer, CountVectorizer\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nimport re\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\n# 5. Unsupervised Clustering (KMeans)\nkmeans = KMeans(featuresCol=\"features\", k=7, predictionCol=\"cluster\")\npipeline = Pipeline(stages=[hashing_tf, idf, kmeans])\nmodel = pipeline.fit(df_cluster)\nclustered_data = model.transform(df_cluster)\n\nevaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"cluster\", metricName=\"silhouette\")\nsilhouette = evaluator.evaluate(clustered_data)"}, {"cell_type": "code", "execution_count": 18, "id": "ad382974-8c11-47be-a6b7-36b950544963", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Silhouette Score: 0.1160237163747209\n"}], "source": "#print(f\"Silhouette Score: {silhouette}\")"}, {"cell_type": "code", "execution_count": 10, "id": "00400dc5-9aeb-4ec5-8491-fc21abc167f0", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import count\n\n#clustered_data.select('cluster').distinct().show()"}, {"cell_type": "code", "execution_count": 19, "id": "0d7b0a65-f9c9-476b-a62c-5ff4d9ae9e7f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/11/26 23:03:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 22.1 MiB\n23/11/26 23:04:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 22.1 MiB\n23/11/26 23:04:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 22.1 MiB\n"}, {"name": "stdout", "output_type": "stream", "text": "+-------+-------------+\n|cluster|cluster_count|\n+-------+-------------+\n|      1|          475|\n|      6|            8|\n|      3|          522|\n|      5|       321466|\n|      4|           17|\n|      8|            1|\n|      7|          146|\n|      2|         5517|\n|      0|      1245640|\n+-------+-------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "23/11/26 23:04:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 22.1 MiB\n"}], "source": "#clustered_data.groupby(\"cluster\").agg(count(\"*\").alias(\"cluster_count\")).show()"}, {"cell_type": "code", "execution_count": null, "id": "2438f3a7-6d75-402d-8286-a98f9d37e81d", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import when\n\nclustered_data = clustered_data.withColumn(\"Genre\", when(clustered_data[\"cluster\"] == 0, \"Romance\")\n                                          .when(clustered_data[\"cluster\"] == 1, \"Literature & Education\")\n                                          .when(clustered_data[\"cluster\"] == 2, \"Religion & Inspirational\")\n                              .when(clustered_data[\"cluster\"] == 3, \"Science Fiction & Fantasy\")\n                              .when(clustered_data[\"cluster\"] == 4, \"Crime & Mystery\")\n                              .when(clustered_data[\"cluster\"] == 5, \"Romance\")\n                              .when(clustered_data[\"cluster\"] == 6, \"Biography & Memoir\")\n                                .when(clustered_data[\"cluster\"] == 7, \"Others\"))"}, {"cell_type": "code", "execution_count": null, "id": "3e84788d-4bde-4169-a352-c69dfd9ad5b5", "metadata": {}, "outputs": [], "source": "#0 - superhero/scif\n#1 - history/education\n#2 - thriller/crime/mystery\n#3 - biography\n#4 - religion/spiritual\n#5 - self-help\n#6 - comedy\n#7 - other (adventure/cooking)\n\nCrime/Thriller/Mystery/Action\nHistory/Education/Literature\nSpiritual/Self Help\nComedy/Satire\nRomance\nOthers\nAutobiograph/Biography/Memoir\nSci-Fi"}, {"cell_type": "code", "execution_count": 10, "id": "55f05cfc-b60f-4397-a494-f6c654ecee60", "metadata": {}, "outputs": [], "source": "final_cluster_df = clustered_data.select('book_id', 'Genre')"}, {"cell_type": "code", "execution_count": 11, "id": "15207882-0565-4624-abe6-cb710908ed7a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/11/26 21:30:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 20.3 MiB\n                                                                                \r"}], "source": "final_cluster_df.write.format(\"bigquery\").option(\"temporaryGcsBucket\",\"msca-bdp-project-goodreads\").option(\"table\", \"msca-bdp-student-ap.Goodreads_Project.genres_cluster\").mode(\"overwrite\").save()"}, {"cell_type": "code", "execution_count": 20, "id": "8c124587-f75c-48a7-83f7-0aaa7f65ca37", "metadata": {}, "outputs": [], "source": "book_genres = spark.read.format(\"bigquery\").option(\"table\", \"msca-bdp-student-ap.Goodreads_Project.genres_cluster\").load()"}, {"cell_type": "code", "execution_count": 24, "id": "1146a1f3-db48-4ab8-9878-394d9887d463", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------+\n|Genre                 |\n+----------------------+\n|History/Education     |\n|Religion/Spiritual    |\n|Biograph              |\n|Thriller/Crime/Mystery|\n|Self Help             |\n|Cooking/Other Hobbies |\n|Comedy                |\n|Sci-Fi                |\n+----------------------+\n\n"}], "source": "book_genres.select('Genre').distinct().show(truncate=False)"}, {"cell_type": "code", "execution_count": 26, "id": "31a6b433-843d-42d2-b91d-a93743a9bc06", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import when\n\nbook_genres = book_genres.withColumn(\"Genre\", when(book_genres[\"Genre\"] == 'Comedy', \"Romance\")\n                                          .when(book_genres[\"Genre\"] == 'History/Education', \"Literature & Education\")\n                                          .when(book_genres[\"Genre\"] == 'Religion/Spiritual', \"Religion & Inspirational\")\n                              .when(book_genres[\"Genre\"] == 'Sci-Fi', \"Science Fiction & Fantasy\")\n                              .when(book_genres[\"Genre\"] == 'Thriller/Crime/Mystery', \"Crime & Mystery\")\n                              .when(book_genres[\"Genre\"] == 'Self Help', \"Romance\")\n                              .when(book_genres[\"Genre\"] == 'Biograph', \"Biography & Memoir\")\n                                .when(book_genres[\"Genre\"] == 'Cooking/Other Hobbies', \"Others\"))"}, {"cell_type": "code", "execution_count": null, "id": "e00d161d-23b2-41d5-95a1-7ee516a88c2a", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}